#!/bin/bash -i
#Necessary packages:-parser, -annotator, BRAKER, samtools, blast, interproscan, etc.#
#Put input genome in a folder named genome#
import os
genome="genome/"
configfile: "FindPlantNLRs.config"
SAMPLES = list(set([x.split(".")[0] for x in os.listdir(genome) if x.endswith("fa")]))
path = 'tmp'
if not os.path.exists(path):
       os.mkdir(path)
result = 'result'
if not os.path.exists(result):
       os.mkdir(result)
rule all:
     input:
         expand('tmp/{sample}.all_20kbflanking_merged_upper.fasta', sample=SAMPLES)
#Chopping the genome sequence into overlapping subsequences#
rule chop_sequence:
     input:
         "genome/{sample}.fa"
     output:
         "tmp/{sample}.choppedseq.fa"
     shell:
         "java -jar {config[NLR-Annotator]}/ChopSequence.jar -i {input} -o {output} -l 20000 -p 5000"
#Searching the chopped subsequences for pre-determined -associated motifs#
rule search_motifs:
     input:
         "tmp/{sample}.choppedseq.fa"
     output:
         "tmp/{sample}.parser.xml"
     shell:
         "java -jar {config[NLR-Annotator]}/NLR-Parser3.jar -t 10 -y {config[meme]} -x {config[meme_xml]} -i {input} -c {output}"
#Generate the GFF format of  loci for the searched motifs#
rule run_NLR_annotator:
     input:
         "tmp/{sample}.parser.xml"
     output:
         "tmp/{sample}.parser.gff"
     shell:
         "java -jar {config[NLR-Annotator]}/NLR-Annotator.jar -i {input} -g {output}"
#Indexing genome sequence#
rule index_genome:
     input:
         "genome/{sample}.fa"
     output:
         "genome/{sample}.fa.fai"
     shell:
         "samtools faidx {input}"
#Creat a file which have contig length information.
rule get_contig_length:
     input:
         "genome/{sample}.fa.fai"
     output:
         "genome/{sample}.genomefile"
     shell:
         "cut -d $'\t' -f1,2 {input} > {output}"
#Convert format of parser.gff in order to incorporate with hmm output later 
rule convert_paser_format:
      input:
          "tmp/{sample}.parser.gff"
      output:
          "tmp/{sample}.parser.bed"
      shell:
          """awk -v OFS='\\t' '{{if ($7 == "+") {{print $1, $4, $5, $1, "forward", $7}} else if ($7 == "-") print $1, $4, $5, $1, "reverse", $7}}' {input} >{output}"""
#Convert to 20kbflanking bed file with bedtools#
rule get_paser_20kbflanking:
       input:
          bed="tmp/{sample}.parser.bed",
          genomefile="genome/{sample}.genomefile"
       output:
          "tmp/{sample}_parser.20kbflanking.bed"
       shell:
          "bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} | bedtools sort -i - |bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct, > {output}"
#-------------------------------------------Use blast to identify genes which cannot be detected by  annotator pipeline------------------------------------------
#-----------------------------------------------------------------part 2--------------------------------------------------------------------------------------------
#Make a genome database for detecting nucleotide or protein query sequence#
#Build blastdb and dectect whether there are genes which cannot be captured by using -parser by using tblastn# remember to form a folder which include blastprotein#
rule run_tblastn:
     input:
         genome="genome/{sample}.fa"
     output:
         "tmp/{sample}.tblastnout.outfmt6"
     params:
         "tmp/{sample}.database"
     run:
         shell("makeblastdb -in {input.genome} -dbtype nucl -parse_seqids -out {params}")
         shell("tblastn -query {config[ref]} -db {params} -num_threads {config[blast_threads]} -evalue 0.001 -outfmt 6 -out {output}")
#Convert tblastn file into bed, and add two coloums for strand information#
rule convert_tblastn_format:
     input:
         "tmp/{sample}.tblastnout.outfmt6"
     output:
         "tmp/{sample}.tblastnout.bed"
     shell:
         """awk -v OFS='\\t' '{{if ($10 - $9 > 0) {{print $2, $9, $10, $1, "forward", "+"}} else if ($10 - $9 < 0) print $2, $10, $9, $1, "reverse", "-"}}' {input} > {output}"""
##Generate 20kb flanking bed file for blast
rule get_blast_20kb:
     input:
         bed="tmp/{sample}.tblastnout.bed",
         genomefile="genome/{sample}.genomefile"
     output:
         "tmp/{sample}.blast.20kbflanking.bed"
     shell:
         """ bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} | bedtools sort -i - | bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct,  >  {output}"""
#Adapted from Peri Tobias' s scripts------------------------------------------------------------------------------------------------
#Use nhmmer to search for conserved nucleotide binding domain shared by Apaf-1, Resistance proteins and CED4 from coiled-coil  and TIR  sequences#
#-----------------------------------------------------------------part 3--------------------------------------------------------------------------------------------
#Peri has already prepared hmm profiles which are named as EG_nonTIRhmm and EG_TIRhmm respectively. 
rule find_TIR:
     input:
         nonTIR="ref_db/EG_nonTIRhmm",
         TIR="ref_db/EG_TIRhmm",
         genome="genome/{sample}.fa"
     output:
         TIR_out="tmp/{sample}.TIRout",
         nonTIR_out="tmp/{sample}.nonTIRout"
     run:
         shell("nhmmer --dna {input.nonTIR} {input.genome} > {output.nonTIR_out}")
         shell("nhmmer --dna {input.TIR} {input.genome} > {output.TIR_out}")
#convert both nhmmer output into bed file by using awk script#
rule awk_convert:
     input:
         TIR_out="tmp/{sample}.TIRout",
         nonTIR_out="tmp/{sample}.nonTIRout"
     output:
         TIRoutbed="tmp/{sample}.TIRout.bed",
         nonTIRoutbed="tmp/{sample}.nonTIRout.bed"
     run:
         shell("""cat {input.TIR_out}|awk '/\Scores for complete hits/{{flag=1;next}}/\------ inclusion threshold ------/{{flag=0}}flag'|awk '(NR >2)'|awk -v OFS='\\t' '{{if ($6 - $5 > 0) {{print $4, $5, $6, $4, "forward", "+"}} else if ($6 - $5 < 0) print $4, $6, $5, $4, "reverse", "-"}}' > {output.TIRoutbed}""")
         shell("""cat {input.nonTIR_out}|awk '/\Scores for complete hits/{{flag=1;next}}/\------ inclusion threshold ------/{{flag=0}}flag'|awk '(NR >2)'|awk -v OFS='\\t' '{{if ($6 - $5 > 0) {{print $4, $5, $6, $4, "forward", "+"}} else if ($6 - $5 < 0) print $4, $6, $5, $4, "reverse", "-"}}' > {output.nonTIRoutbed}""")
#convert both bed file into fasta file by using bedtools#
rule get_fasta_sequence:
     input:
         TIR_bed="tmp/{sample}.TIRout.bed",
         nonTIR_bed="tmp/{sample}.nonTIRout.bed",
         genome="genome/{sample}.fa"
     output:
         TIR_fasta="tmp/{sample}.TIR.fasta",
         nonTIR_fasta="tmp/{sample}.nonTIR.fasta"
     run:
         shell("bedtools getfasta -s -fi {input.genome} -bed {input.TIR_bed} -fo {output.TIR_fasta}")
         shell("bedtools getfasta -s -fi {input.genome} -bed {input.nonTIR_bed} -fo {output.nonTIR_fasta}")
#Extract first 200 sequences from previously output nonTIR and TIR fasta file, change 200 into other number when neccessary (why?#
rule awk:
     input:
         nonTIR="tmp/{sample}.nonTIR.fasta",
         TIR="tmp/{sample}.TIR.fasta"
     output:
         nonTIR_200="tmp/{sample}.nonTIR_200.fasta",
         TIR_200="tmp/{sample}.TIR_200.fasta",
         NBARC400="tmp/{sample}_NBARC_400.fasta"
     run:
         shell("""awk "/^>/ {{n++}} n>200 {{exit}} 1" {input.nonTIR} > {output.nonTIR_200}""") 
         shell("""awk "/^>/ {{n++}} n>200 {{exit}} 1" {input.TIR} > {output.TIR_200}""")
         shell("cat {output.nonTIR_200} {output.TIR_200} > {output.NBARC400}")
#Remove duplicated sequences#
rule run_seqkit:
     input:
         "tmp/{sample}_NBARC_400.fasta"
     output:
         "tmp/{sample}_NBARC.fasta"
     shell:
         "seqkit rmdup -D duplicates -n {input} > {output}"
#Generate alignment by using clustalo#
rule run_clustalo:
     input:
         "tmp/{sample}_NBARC.fasta"
     output:
         "tmp/{sample}_NBARC.sto"
     shell:
         "clustalo -i {input} -o {output} --outfmt=st"
#Build a profile hmm from an alignment#
rule run_hmmbuild:
     input:
         "tmp/{sample}_NBARC.sto"
     output:
         "tmp/{sample}.hmm"
     shell:
         "hmmbuild -nucleic {output} {input}"
#Use hmm profile built to search queries against genome#
rule run_nhmmer:
     input:
         hmm="tmp/{sample}.hmm",
         genome="genome/{sample}.fa"
     output:
         "tmp/{sample}_NBARCout"
     shell:
         "nhmmer --dna {input.hmm} {input.genome} > {output}"
#Convert nhmmer output into bed file#
#Required double-check#
rule make_bed_hmmout:
     input:
         NBARC="tmp/{sample}_NBARCout"
     output:
         "tmp/{sample}_NBARC.bed"
     shell:
         """cat {input.NBARC}|awk '/\Scores for complete hits/{{flag=1;next}}/\------ inclusion threshold ------/{{flag=0}}flag'|awk '(NR >2)'|awk -v OFS='\\t' '{{if ($6 - $5 > 0) {{print $4, $5, $6, $4, "forward", "+"}} else if ($6 - $5 < 0) print $4, $6, $5, $4, "reverse", "-"}}' > {output}"""     
#Get 20kb upstream and downstream# 
rule get_NBARC_20kb:
     input:
         bed="tmp/{sample}_NBARC.bed",
         genomefile="genome/{sample}.genomefile"
     output:
         "tmp/{sample}_NBARC.20kbflanking.bed"
     shell:
         "bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} >  {output}"
#---------------------------------------Now we have output from hmm, blast and _annotator, combine them into one file--------------------------------------------
#-----------------------------------------------------------------part 4--------------------------------------------------------------------------------------------
##Combine the output together#
#Include blast file after getting the query file#
rule combine_all_bed:
     input:
         hmm="tmp/{sample}_NBARC.20kbflanking.bed",
         annotator="tmp/{sample}_parser.20kbflanking.bed",
         blast="tmp/{sample}.blast.20kbflanking.bed"
     output:
         "tmp/{sample}_all20kbflanking.bed"
     shell:
         """cat {input.hmm} {input.annotator} {input.blast}|awk -v  OFS='\\t' '{{print $1,$2,$3}}'  |sort -k1,1 -k2,2n > {output}"""
#Merge the bed file now#
rule merge_all_20kbflanking:
     input:
         "tmp/{sample}_all20kbflanking.bed"
     output:
         "tmp/{sample}.all_20kbflanking_merged.bed"
     shell:
#         "bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct, -i {input}|awk -v OFS='\\t' '{{print $1,$2,$3}}'|sort -k1,2 |uniq  > {output}"
#         "bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct, -i {input} |sort -k1,2 |uniq  > {output}"
         "bedtools merge -i {input} > {output}"
#Convert bedfile into fasta#
rule convert_20kbflankingbedfile_fasta:
     input:
         genome="genome/{sample}.fa",
         bed="tmp/{sample}.all_20kbflanking_merged.bed"
     output:
         "tmp/{sample}.all_20kbflanking_merged.fasta"
     shell:
         "bedtools getfasta -fi {input.genome} -bed {input.bed} -fo {output}"	 
#Convert all the sequences in 20kb flanking fasta into uppercase (not sure)#
rule convert_format:
     input:
         "tmp/{sample}.all_20kbflanking_merged.fasta"
     output:
         "tmp/{sample}.all_20kbflanking_merged_upper.fasta"
     shell:
         """cat {input} | awk "/^>/ {{print(\$0)}}; /^[^>]/ {{print(toupper(\$0))}}" > {output}"""
		



