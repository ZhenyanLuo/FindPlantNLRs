#Necessary packages:NLR-parser, NLR-annotator, BRAKER, samtools, blast, interproscan, etc.#
#use bash conda_install.sh command to install neccessary modules#
#Adapted from Tamene Tolessa's script#
SAMPLES = ["A_floribunda_di", "C_calophylla_di", "C_maculata_di", "E_albens_di", "E_brandiana_di", "E_caleyi_di", "E_camaldulensis_di", "E_cladocalyx_di", "E_cloeziana_di", "E_coolabah_di", "E_curtisii_di", "E_dawsonii_di", "E_decipiens_di", "E_erythrocorys_di", "E_fibrosa_di", "E_globulus_di", "E_grandis_di", "E_guilfoylei_di", "E_lansdowneana_di", "E_leucophloia_di", "E_marginata_di", "E_melliodora_di", "E_melliodora_x_E_sideroxylon_di", "E_microcorys_di", "E_paniculata_di", "E_pauciflora_di", "E_polyanthemos_di", "E_pumila_di", "E_regnans_di", "E_shirleyi_di", "E_sideroxylon_di", "E_tenuipes_di", "E_victrix_di", "E_viminalis_di", "E_virginea_di", "M_polymorpha", "P_guajava", "R_argentra", "Rpsi_hifiasm_p_utg", "R_rubescens", "S_oleosum"]
#Making a temp folder#
import os
path = 'tmp'
if not os.path.exists(path):
       os.mkdir(path)
result = 'result'
if not os.path.exists(result):
       os.mkdir(result)
#Remember to correct this since this should be the final output#
rule all:
     input:
#         expand('tmp/{sample}_braker_modified.gtf', sample=SAMPLES),
#         expand('tmp/{sample}.NB-ARC_hmmsearch.tsv', sample=SAMPLES),
#         expand('tmp/{sample}.choppedseq.fa', sample=SAMPLES),  
#         expand('tmp/{sample}.NB-ARC_hmmsearch.gff3', sample=SAMPLES),
         expand('tmp/{sample}.all_20kbflanking_merged_upper.fasta', sample=SAMPLES),
#         expand('tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa', sample=SAMPLES),
#         expand('tmp/{sample}_all_20kbflanking_removed.fasta', sample=SAMPLES)
#         expand('braker/{sample}_augustus.gtf', sample=SAMPLES),
#         expand('braker/{sample}_braker.gtf', sample=SAMPLES)
#Chopping the genome sequence into overlapping subsequences#
rule chop_sequence:
     input:
         "genome/{sample}.fa"
     output:
         "tmp/{sample}.choppedseq.fa"
     priority:100
     shell:
         "java -jar NLR-Annotator/ChopSequence.jar -i {input} -o {output} -l 20000 -p 5000"
#Searching the chopped subsequences for pre-determined NLR-associated motifs#
rule search_NLR_motifs:
     input:
         "tmp/{sample}.choppedseq.fa"
     output:
         "tmp/{sample}.NLRparser.xml"
     priority:99
     shell:
         "java -jar NLR-Annotator/NLR-Parser3.jar -t 10 -y ../software/meme/bin/mast -x NLR-Annotator/meme.xml -i {input} -c {output}"
#Generate the GFF format of NLR loci for the searched motifs#
rule NLR_annotator:
     input:
         "tmp/{sample}.NLRparser.xml"
     output:
         "tmp/{sample}.NLRparser.gff"
     priority:98
     shell:
         "java -jar NLR-Annotator/NLR-Annotator.jar -i {input} -g {output}"
#Indexing reference sequence#
rule index_ref_seq:
     input:
         "genome/{sample}.fa"
     output:
         "genome/{sample}.fa.fai"
     priority:97
     shell:
         "samtools faidx {input}"
#Create genome file. (?)#
rule convert_genome_format:
     input:
         "genome/{sample}.fa.fai"
     output:
         "genome/{sample}.genomefile"
     priority:96
     shell:
         "cut -d $'\t' -f1,2 {input} > {output}"
#Convert format of NLRparser.gff in order to incorporate with hmm output later# 
rule convert_NLRpaser:
      input:
          "tmp/{sample}.NLRparser.gff"
      output:
          "tmp/{sample}.NLRparser.bed"
      priority:95
      shell:
          """awk -v OFS='\\t' '{{if ($7 == "+") {{print $1, $4, $5, $1, "forward", $7}} else if ($7 == "-") print $1, $4, $5, $1, "reverse", $7}}' {input} >{output}"""
#Convert to 20kbflanking bed file with bedtools#
rule NLRpaser_20kbflanking:
       input:
          bed="tmp/{sample}.NLRparser.bed",
          genomefile="genome/{sample}.genomefile"
       output:
          "tmp/{sample}_NLRparser.20kbflanking.bed"
       priority:94
       shell:
          "bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} | bedtools sort -i - |bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct, > {output}"
#Part 1 already tested and passed#              
              
#-------------------------------------------Use blast to identify genes which cannot be detected by NLR annotator pipeline------------------------------------------
#-----------------------------------------------------------------part 2--------------------------------------------------------------------------------------------
#Make a genome database for detecting nucleotide or protein query sequence#
#Build blastdb and dectect whether there are genes which cannot be captured by using NLR-parser by using tblastn# remember to form a folder which include blastprotein#
rule run_tblastn:
     input:
         genome="genome/{sample}.fa",
         ref="genome/RefPlant_235NLR_ADR1_ZAR1_protein.fa"
     output:
         outfmt6="tmp/{sample}.tblastnout.outfmt6"
     priority:93
     params:
         "tmp/{sample}.database"
     run:
         shell("makeblastdb -in {input.genome} -dbtype nucl -parse_seqids -out {params}")
         shell("tblastn -query {input.ref} -db {params} -evalue 0.001 -outfmt 6 -out {output.outfmt6}")
#Convert tblastn file into bed, and add two coloums for strand information#
rule tblastn_to_bed:
     input:
         "tmp/{sample}.tblastnout.outfmt6"
     output:
         "tmp/{sample}.tblastnout.bed"
     priority:92
     shell:
         """awk -v OFS='\\t' '{{if ($10 - $9 > 0) {{print $2, $9, $10, $1, "forward", "+"}} else if ($10 - $9 < 0) print $2, $10, $9, $1, "reverse", "-"}}' {input} > {output}"""
##Generate 20kb flanking bed file for blast 
rule blast_20kb:
     input:
         bed="tmp/{sample}.tblastnout.bed",
         genomefile="genome/{sample}.genomefile"
     output:
         "tmp/{sample}.blast.20kbflanking.bed"
     priority:91
     shell:
         """ bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} | bedtools sort -i - | bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct,  >  {output}"""      
#Adapted from Peri Tobias' s scripts------------------------------------------------------------------------------------------------
#Use nhmmer to search for conserved nucleotide binding domain shared by Apaf-1, Resistance proteins and CED4 from coiled-coil NLR and TIR NLR sequences#
#-----------------------------------------------------------------part 3--------------------------------------------------------------------------------------------
#Peri has already prepared hmm profiles which are named as EG_nonTIRhmm and EG_TIRhmm respectively. 
rule find_TIR:
     input:
         nonTIR="genome/EG_nonTIRhmm",
         TIR="genome/EG_TIRhmm",
         genome="genome/{sample}.fa"
     output:
         TIR_out="tmp/{sample}.TIRout",
         nonTIR_out="tmp/{sample}.nonTIRout"
     priority:90
     run:
         shell("nhmmer {input.nonTIR} {input.genome} > {output.nonTIR_out}")
         shell("nhmmer {input.TIR} {input.genome} > {output.TIR_out}")
#convert both nhmmer output into bed file by using awk script#
rule awk_convert:
     input:
         TIR_out="tmp/{sample}.TIRout",
         nonTIR_out="tmp/{sample}.nonTIRout"
     output:
         TIRoutbed="tmp/{sample}.TIRout.bed",
         nonTIRoutbed="tmp/{sample}.nonTIRout.bed"
     priority:89
     run:
         shell("awk -f Peris_NLR/Myrtaceae_NLR_workflow/make_bed_hmmOut.awk {input.TIR_out} > {output.TIRoutbed}")
         shell("awk -f Peris_NLR/Myrtaceae_NLR_workflow/make_bed_hmmOut.awk {input.nonTIR_out} > {output.nonTIRoutbed}")
#convert both bed file into fasta file by using bedtools#
rule bedtools:
     input:
         TIR_bed="tmp/{sample}.TIRout.bed",
         nonTIR_bed="tmp/{sample}.nonTIRout.bed",
         genome="genome/{sample}.fa"
     output:
         TIR_fasta="tmp/{sample}.TIR.fasta",
         nonTIR_fasta="tmp/{sample}.nonTIR.fasta"
     priority:88
     run:
         shell("bedtools getfasta -s -fi {input.genome} -bed {input.TIR_bed} -fo {output.TIR_fasta}")
         shell("bedtools getfasta -s -fi {input.genome} -bed {input.nonTIR_bed} -fo {output.nonTIR_fasta}")
#Extract first 200 sequences from previously output nonTIR and TIR fasta file, change 200 into other number when neccessary (why?#
rule awk:
     input:
         nonTIR="tmp/{sample}.nonTIR.fasta",
         TIR="tmp/{sample}.TIR.fasta"
     output:
         nonTIR_200="tmp/{sample}.nonTIR_200.fasta",
         TIR_200="tmp/{sample}.TIR_200.fasta",
         NBARC400="tmp/{sample}_NBARC_400.fasta"
     priority:87
     run:
         shell("""awk "/^>/ {{n++}} n>200 {{exit}} 1" {input.nonTIR} > {output.nonTIR_200}""") 
         shell("""awk "/^>/ {{n++}} n>200 {{exit}} 1" {input.TIR} > {output.TIR_200}""")
         shell("cat {output.nonTIR_200} {output.TIR_200} > {output.NBARC400}")
#Remove duplicated sequences#
rule seqkit:
     input:
         "tmp/{sample}_NBARC_400.fasta"
     output:
         "tmp/{sample}_NBARC.fasta"
     priority:86
     shell:
         "seqkit rmdup -D duplicates -n {input} > {output}"
#Generate alignment by using clustalo#
rule clustalo:
     input:
         "tmp/{sample}_NBARC.fasta"
     output:
         "tmp/{sample}_NBARC.sto"
     priority:85
     shell:
         "clustalo -i {input} -o {output} --outfmt=st"
#Build a profile hmm from an alignment#
rule hmmbuild:
     input:
         "tmp/{sample}_NBARC.sto"
     output:
         "tmp/{sample}.hmm"
     priority:84
     shell:
         "hmmbuild -nucleic {output} {input}"
#Use hmm profile built to search queries against genome#
rule nhmmer:
     input:
         hmm="tmp/{sample}.hmm",
         genome="genome/{sample}.fa"
     output:
         "tmp/{sample}_NBARCout"
     priority:83
     shell:
         "nhmmer {input.hmm} {input.genome} > {output}"
#Convert nhmmer output into bed file#
#Required double-check#
rule make_bed_hmmout:
     input:
         NBARC="tmp/{sample}_NBARCout"
     output:
         "tmp/{sample}_NBARC.bed"
     priority:82
     shell:
         "awk -f Peris_NLR/Myrtaceae_NLR_workflow/make_bed_hmmOut.awk {input.NBARC} > {output}"     
#Get 20kb upstream and downstream# 
rule NBARC_20flanking:
       input:
          bed="tmp/{sample}_NBARC.bed",
          genomefile="genome/{sample}.genomefile"
       output:
          "tmp/{sample}_NBARC.20kbflanking.bed"
       priority:81
       shell:
          "bedtools slop -b 20000 -s -i {input.bed} -g {input.genomefile} >  {output}"
#---------------------------------------Now we have output from hmm, blast and NLR_annotator, combine them into one file--------------------------------------------
#-----------------------------------------------------------------part 4--------------------------------------------------------------------------------------------
##Combine the output together#
#Include blast file after getting the query file#
rule combine_all_bed:
       input:
           hmm="tmp/{sample}_NBARC.20kbflanking.bed",
           annotator="tmp/{sample}_NLRparser.20kbflanking.bed",
           blast="tmp/{sample}.blast.20kbflanking.bed"
       output:
           "tmp/{sample}_all20kbflanking.bed"
       priority:80
       shell:
           "cat {input.hmm} {input.annotator} {input.blast}|sort -k1,1 -k2,2n >{output}"
#Merge the bed file now#
rule merge_all_20kbflanking:
     input:
         "tmp/{sample}_all20kbflanking.bed"
     output:
         "tmp/{sample}.all_20kbflanking_merged.bed"
     priority:79
     shell:
         """bedtools merge -s -d 1 -c 1,5,6 -o distinct,distinct,distinct, -i {input}|awk -v OFS='\\t' '{{print $1,$2,$3}}'|sort -k1,2 |uniq  > {output}"""               
#Convert bedfile into fasta#
rule convert_20kbflankingbedfile_fasta:
     input:
         genome="genome/{sample}.fa",
         bed="tmp/{sample}.all_20kbflanking_merged.bed"
     output:
          "tmp/{sample}.all_20kbflanking_merged.fasta"
     priority:78
     shell:
          "bedtools getfasta -fi {input.genome} -bed {input.bed} -fo {output}"
#Convert all the sequences in 20kb flanking fasta into uppercase (not sure)#
rule convert_format:
     input:
         "tmp/{sample}.all_20kbflanking_merged.fasta"
     output:
         "tmp/{sample}.all_20kbflanking_merged_upper.fasta"
     shell:
         "cat {input} | awk '/^>/ {{print($0)}}; /^[^>]/ {{print(toupper($0))}}' > {output}"   
#Maybe use 20kbflanking.fa instead of NBARC_nt.fasta#
#Translate nucleotide NBARC sequeces including extended sequences#
#rule translate:
#     input:
#         "tmp/{sample}.all_20kbflanking_merged_upper.fasta"
#     output: 
#         "tmp/{sample}.all_20kbflanking_merged_upper.faa"
#     shell:  
#         "Peris_NLR/Myrtaceae_NLR_workflow/translate.py {input} {output}"
#----------------------------------To classify the output of annotator and hmm#
#Remove * in stop codon, otherwise interproscan will not work#
#rule remove_stop_codon:
#     input:
#         "tmp/{sample}.all_20kbflanking_merged_upper.faa"
#     shell:
#         "sed -i 's/*//g' {input}"
#Run Interproscan, database options: Pfam, coils, gene3D #
#rule Interproscan:
#     input:
#         "tmp/{sample}.all_20kbflanking_merged_upper.faa"
#     params:
#         "tmp/"
#     shell:
#          "./interproscan/interproscan-5.50-84.0/interproscan.sh -t p -appl Pfam,COILS,Gene3D -i {input} -f tsv,gff3 -d {params}"
#Predict genes by using braker, remove special header first##Remember to use extended one#
#RefPlantNLR_aa.fa is from https://www.biorxiv.org/content/10.1101/2020.07.08.193961v2#
#Remember to correct the path for braker.pl#
####Please double check#
#This step is modified from Peri's script: braker_nlr.pbs#
#Remove sample species from config file of braker if you stopped once#
#rule braker:
#    input:
#        raw="tmp/{sample}.all_20kbflanking_merged_upper.fasta",
#        ref="genome/prothint_sequences.fa"
#    output:
#        removed="tmp/{sample}_all_20kbflanking_removed.fasta",
#        hints_gtf="braker/{sample}_braker.gtf",
#        gff3="braker/{sample}_augustus.gtf"
#    params:
#        "{sample}"
#    run:
#        shell("sed 's/(//;s/)//' {input.raw} > {output.removed}")
#        shell("./braker_2.1.6/BRAKER/scripts/braker.pl --cores=15 --genome={output.removed} --prot_seq={input.ref} --species={params} --gff3")
#        shell("mv braker/augustus.hints.gtf {output.gff3}")
#        shell("mv braker/braker.gtf {output.hints_gtf}")
#        shell("rm -r braker/GeneMark-EP braker/GeneMark-ES")
#Use getAnnoFastaFromJoingenes.py in augustus to get both nucleotide and protein sequence from braker.gtf#
#rule braker_gff_to_fasta:
#     input:
#         genome="tmp/{sample}_all_20kbflanking_removed.fasta",
#         bed="braker/{sample}_braker.gtf"
#     output:
#         aa="tmp/{sample}_braker.aa",
#         codingseq="tmp/{sample}_braker.codingseq",
#         modified="tmp/{sample}_braker_modified.gtf",
#         genemark="tmp/{sample}_braker_genemark.gtf",
#         augustus="tmp/{sample}_braker_augustus.gtf"    
#     params:
#         "tmp/tmp_braker"
#     run:
#         shell("./Augustus/scripts/getAnnoFastaFromJoingenes.py -g {input.genome} -f {input.bed} -o {params}")
#         shell("mv {params}.aa {output.aa}")
#         shell("mv {params}.codingseq {output.codingseq}")
#         shell("sed -i 's/file_1_//g' {output.aa} {output.codingseq}")
#         shell("sed -i 's/file_2_//g' {output.aa} {output.codingseq}")
#         shell("""cat {input.bed}| grep "GeneMark.hmm" | awk -v OFS='\\t' '{{print $1,$2,$3,$4,$5,$6,$7,$8,$11,$12,$9,$10}}' > {output.genemark}""")
#         shell("grep 'AUGUSTUS' {input.bed} > {output.augustus}")
#         shell("cat {output.genemark} {output.augustus} >{output.modified}")
#         shell("sed -i 's/_t/.t/g; s/_g/.g/g' {output.modified}")
#Remove special characters and rename the augustus output#
#rule braker_step2:
#     input:
#         "tmp/{sample}_braker.aa"
#     output:
#         "tmp/{sample}_braker.faa"
#     shell:
#         "sed s/\*//g {input} > {output}"
###Tamene's version, use PF00931 and Pfam-A.hmm----------------------------------------------------------------
#Let's start from using hmm profile built ({sample}.hmm) #
#rule hmmsearch:
#     input:
#          "tmp/{sample}_braker.faa"
#     output:
#          noali="tmp/{sample}.NB-ARC_tblout_noali.txt",
#          tblout="tmp/{sample}.NB-ARC_hmmsearch_tblout.perseqhit.txt"
#     params:
#          "Pfam/PF00931.hmm"
#     shell:
#          "hmmsearch -o {output.noali} --tblout {output.tblout} --noali --notextw {params} {input}"
#Sorting out and cutting of sequence IDs of domains
#rule grep_hmmsearch:
#     input:
#          "tmp/{sample}.NB-ARC_hmmsearch_tblout.perseqhit.txt"
#     output:
#          "tmp/{sample}.NB-ARC_hmmsearch_perseqhit_seqID.txt"
#     shell:
#          "grep -v '#' {input} | sort -k5,5n | cut -d ' ' -f1 | uniq > {output}"
#Using the sequence ID and seqtk to pull out protein sequences of NB-ARC domains
#rule seqtk:
#     input:
#           seqID="tmp/{sample}.NB-ARC_hmmsearch_perseqhit_seqID.txt",
#           hint="tmp/{sample}_braker.faa"
#     output:
#           "tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa"
#     shell:   
#           "seqtk subseq {input.hint} {input.seqID} > {output}"
#Removing asterisk (*) from the end of each sequences
#rule sed:
#     input:
#           "tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa"    
#     shell:
#           "sed -i 's/*//g' {input}"
#Using InterProScan to detect the TIR, LRR and COIL sub-classes of NB-ARC domains
#Remember to update the path of interproscan.sh if new version of interproscan is available
#rule interproscan_NBARC:
#     input:
#           "tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa"
#     output:
#           gff3="tmp/{sample}.NB-ARC_hmmsearch.gff3",
#           tsv="tmp/{sample}.NB-ARC_hmmsearch.tsv"
#     run:
#           shell("./interproscan/interproscan-5.50-84.0/interproscan.sh -t p -appl Pfam, COILS, Gene3D -i {input} -cpu 16 -f tsv, gff3 -d tmp/")
#           shell("mv {input}.gff3 {output.gff3}")
#           shell("mv {input}.tsv {output.tsv}")
#Search NB-ARC domain against library of Pfam
#wget http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz
#wget http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.dat.gz
#wget http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/active_site.dat.gz
#gunzip *.gz
#hmmpress Pfam-A.hmm
#rule pfam_scan:
#     input:
#           "tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa"
#     output:
#           Pfamscan="tmp/{sample}.protein.fa_pfamscan.txt"
#     params:
#           "Pfam"
#     run:
#           shell("./Pfam/PfamScan/pfam_scan.pl -fasta {input} -dir {params} -as -cpu 16 -outfile {output.Pfamscan}")
#Parsing the output of PfamScan output parser using the script
#K-parse_Pfam_domains_v3.1.pl from https://github.com/krasileva-group/plant_rgenes is used in this step, ref: https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-016-0228-7
#rule K_parse:
#     input:
#           "tmp/{sample}.protein.fa_pfamscan.txt"
#     output:
#           "tmp/{sample}.protein.fa_pfamscan-0-.parsed.verbose"
#     shell:
#           "perl ~/nlr_annotation_scripts/K-parse_Pfam_domains_v3.1.pl --pfam {input} --evalue 0.001 --output {output} --verbose T"
#Parsing the output of PfamScan output parser using the script "K-parse_Pfam_domains_NLR-fusions-v2.4.2.pl"#
#K-parse_Pfam_domains_NLR-fusions-v2.2.pl from https://github.com/krasileva-group/plant_rgenes is used in this step, ref: https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-016-0228-7               
#Remember to make a db_descriptions.txt file in genome folder
#Replace Species_ID with Species, and make species name same as sample name#
#rule K_parse_fusion:
#      input:
#           verbose="tmp/{sample}.protein.fa_pfamscan-0-.parsed.verbose",
#           db="genome/db_descriptions.txt"
#      params:
#           mkdir="tmp/Pfam_{sample}_parser"
#      run:
#           shell("mkdir -p {params.mkdir}")
#           shell("cp {input.verbose} {params.mkdir}/")
#           shell("perl ~/nlr_annotation_scripts/K-parse_Pfam_domains_NLR-fusions-v2.2.pl --indir {params.mkdir} --evalue 0.001 -o {params.mkdir} -d {input.db}")
#use NLR_filter_gene_models.py to filt gene models based on privous output#
#rule NLR_filter_gene_model:
#       input:
#           braker="tmp/{sample}_braker_modified.gtf",
#           tsv="result/Interpro_{sample}/{sample}.NB-ARC_hmmsearch.tsv",
#           all_20kb="tmp/{sample}.all_20kbflanking_merged_upper.fasta",
#           hmmer="tmp/{sample}.NB-ARC_hmmsearch_perseqhit_protein.fa",
#           genomebase="genome/{sample}.fa"             
#       run:
#           shell("python ~/nlr_annotation_scripts/NLR_filter_gene_models.py {input.braker} {input.tsv} {input.all_20kb} {input.hmmer} {input.genomebase}")
